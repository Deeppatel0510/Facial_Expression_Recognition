{"cells":[{"cell_type":"markdown","metadata":{"id":"5z8nt9uSJkGr"},"source":["# Facial Expression Recognition Using CNN"]},{"cell_type":"markdown","metadata":{"id":"ITK54PhVBAry"},"source":["**Facial Expression Recognition** using a **Convolutional Neural Network** delves into the process of interpreting human emotions from images. This tutorial walks you through dataset exploration, model development, training, and evaluation. You'll learn how to design a convolutional neural network and train it to recognize emotions such as happiness and sadness."]},{"cell_type":"markdown","metadata":{"id":"MTVMXP3xqphX"},"source":["## Github Link\n","https://github.com/Deeppatel0510/Facial_Expression_Recognition"]},{"cell_type":"markdown","metadata":{"id":"YwyR3mmzxqE9"},"source":["# Project Goal:\n","**The primary goal of DeepFER:** Facial Emotion Recognition Using Deep Learning is to develop an advanced and efficient system capable of accurately identifying and classifying human emotions from facial expressions in real-time. By leveraging state-of-the-art Convolutional Neural Networks (CNNs) and Transfer Learning techniques, this project aims to create a robust model that can handle the inherent variability in facial expressions and diverse image conditions. The system will be trained on a comprehensive dataset featuring seven distinct emotions: angry, sad, happy, fear, neutral, disgust, and surprise. The ultimate objective is to achieve high accuracy and reliability, making DeepFER suitable for applications in human-computer interaction, mental health monitoring, customer service, and beyond. Through this project, we aim to bridge the gap between cutting-edge AI research and practical emotion recognition applications, contributing to more empathetic and responsive machine interactions with humans.\n","\n","Emotion Classes:\n","* Angry: Images depicting expressions of anger.\n","* Sad: Images depicting expressions of sadness.\n","* Happy: Images depicting expressions of happiness.\n","* Fear: Images depicting expressions of fear.\n","* Neutral: Images depicting neutral, non-expressive faces.\n","* Disgust: Images depicting expressions of disgust.\n","* Surprise: Images depicting expressions of surprise."]},{"cell_type":"markdown","metadata":{"id":"cMRASQYfBhXM"},"source":["### **Import Libraries**\n","\n","This section imports the essential libraries needed for constructing and training a convolutional neural network (CNN) for facial expression recognition.\n","\n","- `os`: Provides functions to interact with the operating system, useful for handling file operations.\n","- `cv2`: OpenCV library for computer vision, used here for processing images.\n","- `numpy`: A library for numerical computing, essential for array manipulations.\n","- `tensorflow`: The TensorFlow library used for deep learning tasks.\n","- `train_test_split` from `sklearn.model_selection`: Splits the dataset into training and testing subsets.\n","- `ImageDataGenerator` from `tensorflow.keras.preprocessing.image`: Generates batches of augmented data for training.\n","- `LabelEncoder` from `sklearn.preprocessing`: Converts categorical labels into numerical format.\n","- `to_categorical` from `keras.utils`: Transforms class labels into a binary class matrix.\n","- `Sequential` from `keras.models`: A linear stack of layers used to build deep learning models.\n","- `Dense`, `Conv2D`, `Dropout`, `BatchNormalization`, `MaxPooling2D`, `Flatten` from `keras.layers`: Various layers used in the CNN architecture.\n","- Optimizers (`Adam`, `RMSprop`, `SGD`) from `keras.optimizers`: Algorithms that adjust model weights during training.\n","- `plt` from `matplotlib.pyplot`: A plotting library for visualizing training and validation curves.\n","- Callbacks (`ModelCheckpoint`, `EarlyStopping`, `ReduceLROnPlateau`) from `keras.callbacks`: Tools used during training to enhance model performance or handle interruptions.\n"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11434,"status":"ok","timestamp":1751472849263,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"},"user_tz":-330},"id":"3sUYvF6u4-Tb","outputId":"26213c19-898e-4674-f2bb-9a78399af253"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n","Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"]}],"source":["!pip install tensorflow"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1751472849275,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"},"user_tz":-330},"id":"pk2buJF5oenp"},"outputs":[],"source":["import os\n","import zipfile\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","import PIL\n","from PIL import Image\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"]},{"cell_type":"markdown","metadata":{"id":"yjMs6gBF5S4I"},"source":["## Mount Google Drive"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4978,"status":"ok","timestamp":1751472854290,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"},"user_tz":-330},"id":"Wv0wolf-pDW2","outputId":"943e7de3-41d4-48b7-a48f-5bb756514969"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"v2XGxMqkpapL","executionInfo":{"status":"ok","timestamp":1751472854297,"user_tz":-330,"elapsed":3,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# # extracting data from zipfile\n","\n","# import zipfile\n","# import os\n","\n","# dataset_path = \"/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/Face Emotion Recognition Dataset.zip\"  # Change this to your dataset location\n","# extract_path = \"/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/dataset\"\n","\n","# with zipfile.ZipFile(dataset_path, 'r') as zip_ref:\n","#     zip_ref.extractall(extract_path)\n","\n","# print(\"Dataset extracted successfully!\")"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"nBX_qJWYqau_","executionInfo":{"status":"ok","timestamp":1751472854305,"user_tz":-330,"elapsed":4,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# # Define dataset path\n","# data_dir = \"/content/drive/MyDrive/Facial_Expression_Recognition/dataset/images\"  # Change to extracted folder"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"fEmgoYSPqg0K","executionInfo":{"status":"ok","timestamp":1751472854311,"user_tz":-330,"elapsed":3,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Define paths\n","train_dir = \"/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/dataset/images/images/train\"\n","val_dir = \"/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/dataset/images/images/validation\"\n","\n","Classes = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n","img_size = 224\n","batch_size = 32"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"cdDgjxJZcdBP","executionInfo":{"status":"ok","timestamp":1751472854317,"user_tz":-330,"elapsed":3,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Dataset Directory\n","data_dir = train_dir"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"gWafwx62ilRE","executionInfo":{"status":"ok","timestamp":1751472854350,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Classes\n","sub_folders = os.listdir(data_dir)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"OBRZU_fzisqq","executionInfo":{"status":"ok","timestamp":1751472854352,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Declaring the lists for images and labels\n","images = []\n","labels = []"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"6N7TZ54civYK","executionInfo":{"status":"ok","timestamp":1751473327494,"user_tz":-330,"elapsed":473140,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Accessing the labels\n","for sub_folder in sub_folders:\n","    label = sub_folder\n","\n","    # Constructing the path to the current sub-folder\n","    path = os.path.join(data_dir, sub_folder)\n","\n","    # Listing all images in the current sub-folder\n","    sub_folder_images = os.listdir(path)\n","\n","    # Accessing the Images\n","    for image_name in sub_folder_images:\n","        # Constructing the path to the current image\n","        image_path = os.path.join(path, image_name)\n","        # Loading the image using OpenCV\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n","        # Appending the image to the list of images\n","        images.append(img)\n","        # Appending the label corresponding to the current sub-folder to the list of labels\n","        labels.append(label)"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"t3ehOepji55a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751473327514,"user_tz":-330,"elapsed":26,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}},"outputId":"d876991e-3a4d-4b79-e183-9ab49075d65c"},"outputs":[{"output_type":"stream","name":"stdout","text":["28821\n"]}],"source":["# Converting the lists of images and labels to NumPy arrays\n","images = np.array(images)\n","labels = np.array(labels)\n","print(len(images))"]},{"cell_type":"markdown","metadata":{"id":"XWRd7o4aDxIw"},"source":["The dataset is divided into training, validation, and test sets using the `train_test_split` function from scikit-learn.\n","\n","- `X_train`, `y_train`: Images and labels for training.\n","- `X_val`, `y_val`: Images and labels for validation.\n","- `X_test`, `y_test`: Images and labels for testing.\n","\n","The data is split with 20% allocated to testing and 10% to validation, based on the original dataset. The `random_state` parameter is set to ensure that the split is reproducible."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"3RlIsHBIi9fQ","executionInfo":{"status":"ok","timestamp":1751473327570,"user_tz":-330,"elapsed":57,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Splitting Dataset into training, validation, and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    images, labels, test_size=0.2, random_state=42)\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"pkaWQ0_WD6iH"},"source":["The `preprocessing` function is created to prepare the input images before passing them into the neural network model.\n","\n","- Normalize the pixel values by dividing them by 255.0, scaling the values to the range [0, 1].\n","- Resize each image to a fixed size of 48x48 pixels using OpenCV's `resize` function.\n","- Reshape the image array to fit the input format expected by the neural network model. The shape is `(batch_size, height, width, channels)`, where `batch_size` is `-1` to allow for a dynamic batch size, and `channels` is set to 1 for grayscale images.\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"W-QcvS5EjA8z","executionInfo":{"status":"ok","timestamp":1751473327574,"user_tz":-330,"elapsed":2,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Preprocess the image\n","def preprocessing(img):\n","    img = img / 255.0\n","    img = cv2.resize(img, (48, 48))\n","    return img.reshape(-1, 48, 48, 1)  # Reshape to match input shape"]},{"cell_type":"markdown","metadata":{"id":"NHQ6x-hUEKrM"},"source":["The code snippet applies the `preprocessing` function to each image in the training, validation, and test sets using the `map` function, and then transforms the resulting list of preprocessed images into NumPy arrays.\n","\n","- `map(preprocessing, X_train)`: Applies the `preprocessing` function to every image in `X_train`.\n","- `list(map(...))`: Converts the map object into a list.\n","- `np.array(...)`: Converts the list of preprocessed images into a NumPy array.\n"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"xYlF0AWcFFVD","executionInfo":{"status":"ok","timestamp":1751473328398,"user_tz":-330,"elapsed":825,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Apply preprocessing to training, validation, and test sets\n","X_train = np.array(list(map(preprocessing, X_train)))\n","X_val = np.array(list(map(preprocessing, X_val)))\n","X_test = np.array(list(map(preprocessing, X_test)))"]},{"cell_type":"markdown","metadata":{"id":"nhlFmsyUFMDy"},"source":["The code reshapes the input data arrays to eliminate an unnecessary dimension. The neural network model expects input in the shape `(batch_size, height, width, channels)`, where `batch_size` denotes the number of samples per batch. The extra dimension is removed to match this expected format.\n","\n","- `reshape(-1, 48, 48, 1)`: Adjusts the input data arrays to a shape of `(batch_size, 48, 48, 1)`, where `-1` allows the batch size to be dynamically set based on the number of samples.\n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"Mfe4OENojGZM","executionInfo":{"status":"ok","timestamp":1751473328448,"user_tz":-330,"elapsed":17,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Reshape input data to remove unnecessary dimension\n","X_train = X_train.reshape(-1, 48, 48, 1)\n","X_val = X_val.reshape(-1, 48, 48, 1)\n","X_test = X_test.reshape(-1, 48, 48, 1)"]},{"cell_type":"markdown","metadata":{"id":"AYvdbcs0Fb_F"},"source":["The `ImageDataGenerator` object is set up with various data augmentation parameters to enhance the training images. Data augmentation is a method used to artificially expand the size of the training dataset by applying random transformations to the images, which helps improve the model's generalization and robustness.\n","\n","- `width_shift_range`: Randomly shifts the image horizontally by a fraction of its width.\n","- `height_shift_range`: Randomly shifts the image vertically by a fraction of its height.\n","- `zoom_range`: Randomly zooms in or out of the image.\n","- `shear_range`: Applies random shear transformations to the image.\n","- `rotation_range`: Rotates the image randomly within a specified angle range.\n","\n","After initializing the `ImageDataGenerator` object, the `fit()` method is called to compute the necessary statistics for data augmentation based on the training data.\n"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"amlCUHiVjMGx","executionInfo":{"status":"ok","timestamp":1751473328775,"user_tz":-330,"elapsed":325,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Initialize ImageDataGenerator for data augmentation\n","data_gen = ImageDataGenerator(\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.1,\n","    shear_range=0.1,\n","    rotation_range=10\n",")\n","\n","# Compute necessary statistics for data augmentation\n","data_gen.fit(X_train)"]},{"cell_type":"markdown","metadata":{"id":"UXPB--KSFqvw"},"source":["The `LabelEncoder` object is initialized to convert class labels into numerical values. This conversion is essential because machine learning models generally require numerical input. The `fit()` method is then called on the `LabelEncoder` to train the encoder with the class labels, allowing it to map the labels to their corresponding numerical values.\n"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"deyiI_MnhUTd","executionInfo":{"status":"ok","timestamp":1751473328789,"user_tz":-330,"elapsed":10,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import tensorflow\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Dropout, BatchNormalization, MaxPooling2D, Flatten\n","from keras.optimizers import Adam, RMSprop, SGD\n","import matplotlib.pyplot as plt\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"v-M6Td56jQnI","executionInfo":{"status":"ok","timestamp":1751473328802,"user_tz":-330,"elapsed":18,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}},"colab":{"base_uri":"https://localhost:8080/","height":80},"outputId":"f6cbe241-a320-4b61-9528-ba2e8a1a148d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LabelEncoder()"],"text/html":["<style>#sk-container-id-2 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-2 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-2 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-2 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-2 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-2 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-2 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-2 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-2 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-2 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-2 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-2 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-2 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-2 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-2 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-2 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-2 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-2 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-2 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-2 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":46}],"source":["# Encode the class labels\n","label_encoder = LabelEncoder()\n","label_encoder.fit(labels)"]},{"cell_type":"markdown","metadata":{"id":"ruMxfBiSF33b"},"source":["The class labels for the training, validation, and test sets are encoded using the `transform()` method of the previously initialized `LabelEncoder` object. This method converts the class labels to their corresponding numerical values based on the mapping established during the fitting process.\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"5-hx7_s8jTZb","executionInfo":{"status":"ok","timestamp":1751473328809,"user_tz":-330,"elapsed":5,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Encode the class labels for training, validation, and test sets\n","y_train = label_encoder.transform(y_train)\n","y_val = label_encoder.transform(y_val)\n","y_test = label_encoder.transform(y_test)"]},{"cell_type":"markdown","metadata":{"id":"7gVmDiaoGHLh"},"source":["The variable `num_classes` is set to the number of unique classes in the dataset, which is obtained from the length of the `classes_` attribute of the `LabelEncoder` object.\n","\n","The `to_categorical()` function is then employed to convert the encoded class labels into one-hot encoded vectors. This conversion is crucial for multi-class classification tasks, where each class label is represented as a binary vector with a 1 in the position corresponding to the class index and 0s in all other positions.\n"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"Pccke-XUjVeQ","executionInfo":{"status":"ok","timestamp":1751473328813,"user_tz":-330,"elapsed":2,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Get the number of classes\n","num_classes = len(label_encoder.classes_)\n","\n","# Convert encoded class labels to one-hot encoded categorical arrays\n","y_train_categorical = to_categorical(y_train, num_classes=num_classes)\n","y_val_categorical = to_categorical(y_val, num_classes=num_classes)\n","y_test_categorical = to_categorical(y_test, num_classes=num_classes)"]},{"cell_type":"markdown","metadata":{"id":"nwv0iuDjGeHw"},"source":["The `build_model` function defines the architecture of the convolutional neural network (CNN) designed for facial expression recognition.\n","\n","- **1st Layer**: Convolutional layer with 64 filters of size (5, 5), ReLU activation, and batch normalization. This layer is followed by MaxPooling and Dropout layers for regularization.\n","\n","- **2nd Layer**: Convolutional layer with 128 filters of size (3, 3), ReLU activation, and batch normalization, with MaxPooling and Dropout layers added for regularization.\n","\n","- **3rd Layer**: Convolutional layer with 512 filters of size (3, 3), ReLU activation, and batch normalization, accompanied by MaxPooling and Dropout layers for regularization.\n","\n","- **4th Layer**: Convolutional layer with 512 filters of size (3, 3), ReLU activation, and batch normalization, followed by MaxPooling and Dropout layers for regularization.\n","\n","- **Flatten Layer**: Flattens the output from the convolutional layers to prepare it for the fully connected layers.\n","\n","- **Fully Connected Layer 1**: Dense layer with 256 units and ReLU activation, including batch normalization and dropout for regularization.\n","\n","- **Fully Connected Layer 2**: Dense layer with 512 units and ReLU activation, with batch normalization and dropout applied for regularization.\n","\n","- **Output Layer**: Dense layer with softmax activation for multi-class classification, where the number of units corresponds to the number of classes in the dataset.\n","\n","- **Compilation**: The model is compiled using the Adam optimizer, categorical cross-entropy loss function, and accuracy metric.\n"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"ebDumsoOjZJC","executionInfo":{"status":"ok","timestamp":1751473328817,"user_tz":-330,"elapsed":2,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Building Model\n","def build_model():\n","    model = Sequential()\n","    # 1st Layer\n","    model.add(Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu', input_shape=(48, 48, 1)))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(Dropout(0.3))\n","\n","    # 2nd Layer\n","    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(Dropout(0.3))\n","\n","    # 3rd layer\n","    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(Dropout(0.3))\n","\n","    # 4th layer\n","    model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(2, 2))\n","    model.add(Dropout(0.3))\n","\n","    # Flatten Layer\n","    model.add(Flatten())\n","\n","    # Fully connected layer 1\n","    model.add(Dense(256, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    # Fully connected layer 2\n","    model.add(Dense(512, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.3))\n","\n","    # Output layer\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    # Compiling the model\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"I3CiJpP4GvHS"},"source":["The `summary()` method is invoked on the constructed model to provide an overview of its architecture. This summary includes details on the layers, their output shapes, and the number of trainable parameters.\n"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"St5xjbyDjjGq","executionInfo":{"status":"ok","timestamp":1751473329250,"user_tz":-330,"elapsed":407,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c3f15940-772c-4d66-a990-9ac5feabcedd"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,664\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │       \u001b[38;5;34m590,336\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │         \u001b[38;5;34m2,048\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,359,808\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,179,904\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m3,591\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,336</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,591</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,348,679\u001b[0m (16.59 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,348,679</span> (16.59 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,344,711\u001b[0m (16.57 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,344,711</span> (16.57 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,968\u001b[0m (15.50 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,968</span> (15.50 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["None\n"]}],"source":["# Build the model\n","model = build_model()\n","\n","# Print model summary\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"wLTQrJYkG_Tg"},"source":["The `ModelCheckpoint` callback is set up to save the model weights during training. It tracks the validation accuracy (`val_acc`) and saves only the best model, as determined by the highest validation accuracy, to the specified file `\"model.h5\"`.\n"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"M-smbpQKkVV7","executionInfo":{"status":"ok","timestamp":1751473329262,"user_tz":-330,"elapsed":6,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Initialize ModelCheckpoint callback\n","# checkpoint = ModelCheckpoint(\"model.h5\", monitor=\"val_acc\", verbose=1, save_best_only=True)\n","# Initialize ModelCheckpoint callback\n","checkpoint = ModelCheckpoint(\"model.keras\", monitor=\"val_acc\", verbose=1, save_best_only=True)"]},{"cell_type":"markdown","metadata":{"id":"oBiZw1EpHkFv"},"source":["The `EarlyStopping` callback is configured to monitor the validation loss (`val_loss`). It halts the training process if the validation loss does not improve for a specified number of epochs (`patience`). This early stopping helps prevent overfitting, and the weights of the best-performing model are restored (`restore_best_weights=True`).\n"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"zuArFO2GkbKM","executionInfo":{"status":"ok","timestamp":1751473329275,"user_tz":-330,"elapsed":6,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Initialize EarlyStopping callback\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    min_delta=0,\n","    patience=3,\n","    verbose=1,\n","    restore_best_weights=True\n",")"]},{"cell_type":"markdown","metadata":{"id":"RTAehL6DHvym"},"source":["The `ReduceLROnPlateau` callback is configured to adjust the learning rate dynamically during training based on the validation loss (`val_loss`). If the validation loss does not show improvement for a defined number of epochs (`patience`), the learning rate is reduced by a specified factor (`factor`). This adjustment helps enhance the training process and prevents the model from getting stuck in local minima.\n"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"FcWqpIDfkdkh","executionInfo":{"status":"ok","timestamp":1751473329289,"user_tz":-330,"elapsed":9,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Initialize ReduceLROnPlateau callback\n","reduce_learningrate = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,\n","    patience=3,\n","    verbose=1,\n","    min_delta=0.0001\n",")"]},{"cell_type":"markdown","metadata":{"id":"tJwx_MScH6Vh"},"source":["The `callbacks_list` is a list that includes the callbacks to be applied during model training. It consists of the `EarlyStopping`, `ModelCheckpoint`, and `ReduceLROnPlateau` callbacks. These callbacks are used to monitor the validation loss, save the best model, and adjust the learning rate, respectively.\n"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"hkFIZipIkgA_","executionInfo":{"status":"ok","timestamp":1751473329302,"user_tz":-330,"elapsed":7,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# List of callbacks\n","callbacks_list = [early_stopping, checkpoint, reduce_learningrate]"]},{"cell_type":"markdown","metadata":{"id":"22-FfpcpIJX8"},"source":["The `compile()` method is invoked on the model to set up the training process. It defines the loss function, optimizer, and evaluation metrics to be used during training.\n","\n","- **Loss Function**: Categorical cross-entropy is selected for handling multi-class classification tasks.\n","- **Optimizer**: The Adam optimizer is used with a learning rate of 0.001.\n","- **Metrics**: Accuracy is chosen as the evaluation metric to track the model's performance throughout training.\n"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"_DgH_tueki6T","executionInfo":{"status":"ok","timestamp":1751473329311,"user_tz":-330,"elapsed":4,"user":{"displayName":"Deep Patel","userId":"02787852529693645602"}}},"outputs":[],"source":["# Compile the model\n","model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer=Adam(learning_rate=0.001),\n","    metrics=['accuracy']\n",")"]},{"cell_type":"markdown","metadata":{"id":"WacGYPD5IlBk"},"source":["The `fit()` method is called on the model to train it using the training data. It accepts the following parameters:\n","\n","- `data_gen.flow(X_train, y_train_categorical, batch_size=128)`: A data generator that produces batches of augmented training data, with on-the-fly data augmentation provided by the `ImageDataGenerator` object defined earlier.\n","- `validation_data=(X_val, y_val_categorical)`: Validation data used to assess the model's performance after each epoch.\n","- `epochs=50`: The total number of epochs for training the model.\n","- `verbose=1`: Determines the verbosity level, with `1` enabling progress bars during training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kzrbdyx_UqG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"62e5d237-128c-45da-c6d9-073a2818be3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 5s/step - accuracy: 0.1826 - loss: 2.4422 - val_accuracy: 0.2079 - val_loss: 2.2836\n","Epoch 2/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m817s\u001b[0m 6s/step - accuracy: 0.2410 - loss: 1.9583 - val_accuracy: 0.2598 - val_loss: 1.9785\n","Epoch 3/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m801s\u001b[0m 6s/step - accuracy: 0.3003 - loss: 1.7994 - val_accuracy: 0.2526 - val_loss: 4.1633\n","Epoch 4/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m779s\u001b[0m 5s/step - accuracy: 0.3444 - loss: 1.6726 - val_accuracy: 0.3259 - val_loss: 2.0630\n","Epoch 5/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m765s\u001b[0m 5s/step - accuracy: 0.3905 - loss: 1.5864 - val_accuracy: 0.3922 - val_loss: 1.5710\n","Epoch 6/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m772s\u001b[0m 5s/step - accuracy: 0.4176 - loss: 1.5168 - val_accuracy: 0.4300 - val_loss: 1.4432\n","Epoch 7/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m777s\u001b[0m 5s/step - accuracy: 0.4325 - loss: 1.4642 - val_accuracy: 0.4894 - val_loss: 1.3315\n","Epoch 8/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m796s\u001b[0m 5s/step - accuracy: 0.4608 - loss: 1.4027 - val_accuracy: 0.4183 - val_loss: 1.4770\n","Epoch 9/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m769s\u001b[0m 5s/step - accuracy: 0.4827 - loss: 1.3480 - val_accuracy: 0.5089 - val_loss: 1.2649\n","Epoch 10/10\n","\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.4898 - loss: 1.3232"]}],"source":["# Training the model\n","history = model.fit(\n","    data_gen.flow(X_train, y_train_categorical, batch_size=128),\n","    validation_data=(X_val, y_val_categorical),\n","    epochs=10,\n","    verbose=1\n",")"]},{"cell_type":"markdown","metadata":{"id":"CUBFBK2qIxj4"},"source":["The trained model is saved to a file named `'modelv1.h5'` using the `save()` method. This file includes the model's architecture, weights, and training configuration, enabling you to reload the model later for inference or additional training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WT1_HPvwMRN"},"outputs":[],"source":["# Save the model with .keras extension\n","model.save(\"/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/Custom_CNN_model.keras\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uF0F95fykoDV"},"outputs":[],"source":["# Saving the model with .h5 extension\n","model.save('/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/Custom_CNN_model.h5')"]},{"cell_type":"markdown","metadata":{"id":"Es_-GrjvJRKq"},"source":["The code snippet plots the model's accuracy over epochs for both training and validation. This visualization helps track the model's performance over time and assists in identifying issues such as overfitting or underfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxmRSt_sxDve"},"outputs":[],"source":["# Plotting model performance\n","plt.plot(history.history['accuracy'], label='train_accuracy', marker='o')\n","plt.plot(history.history['val_accuracy'], label='val_accuracy', marker='o')\n","plt.title('Model Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no_b-AcHn8iv"},"outputs":[],"source":["!pip3 install gradio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHBSPlmdoRd6"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","import gradio as gr\n","import tensorflow as tf\n","import numpy as np\n","from PIL import Image\n","import cv2\n","from tensorflow.keras.preprocessing import image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TxBzioJU0pQ"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R96cVPoloWf7"},"outputs":[],"source":["# Ensure your model is loaded here\n","# model = ... # Load your trained model\n","# Load your pre-trained model\n","model = tf.keras.models.load_model('/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/Custom_CNN_model.keras')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYMM-bIDoiQZ"},"outputs":[],"source":["# Emotion labels dictionary\n","emotion_labels = {'angry': 0, 'disgust': 1, 'fear': 2, 'neutral': 3, 'sad': 4, 'happy': 5, 'surprise': 6}\n","index_to_emotion = {v: k for k, v in emotion_labels.items()}\n","index_to_emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epgsBOodA-g5"},"outputs":[],"source":["def prepare_image(img_pil):\n","    \"\"\"Preprocess the PIL image to fit your model's input requirements.\"\"\"\n","    # Resize the image to 48x48 pixels\n","    img = img_pil.resize((48, 48))\n","\n","    # If the model expects grayscale images, convert the image to grayscale\n","    img = img.convert('L')\n","\n","    # Convert the image to a numpy array\n","    img_array = img_to_array(img)\n","\n","    # Add a batch dimension (i.e., convert the image to a 4D tensor)\n","    img_array = np.expand_dims(img_array, axis=0)\n","\n","    # Rescale pixel values to [0, 1] (normalize the data)\n","    img_array /= 255.0\n","\n","    return img_array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vj_Q9TA-o0DR"},"outputs":[],"source":["# Define the Gradio interface\n","def predict_emotion(image):\n","    # Preprocess the image\n","    processed_image = prepare_image(image)\n","    # Make prediction using the model\n","    prediction = model.predict(processed_image)\n","    # Get the emotion label with the highest probability\n","    predicted_class = np.argmax(prediction, axis=1)\n","    predicted_emotion = index_to_emotion.get(predicted_class[0], \"Unknown Emotion\")\n","    return predicted_emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evqSKDVoo7jJ"},"outputs":[],"source":["interface = gr.Interface(\n","    fn=predict_emotion,  # Your prediction function\n","    inputs=gr.Image(type=\"pil\"),  # Input for uploading an image, directly compatible with PIL images\n","    outputs=\"text\",  # Output as text displaying the predicted emotion\n","    title=\"Emotion Detection\",\n","    description=\"Upload an image and see the predicted emotion.\"\n",")\n","\n","# Launch the Gradio interface\n","interface.launch()"]},{"cell_type":"markdown","metadata":{"id":"joW2NTWHdZib"},"source":["# Git Push"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUDNDb7IdZic"},"outputs":[],"source":["!apt-get install git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ykezf-ZedZic"},"outputs":[],"source":["!git --version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1cS_UTEAdZid"},"outputs":[],"source":["!git config --global user.name \"Deeppatel0510\"\n","!git config --global user.email \"deeppatel0510.dp@gmail.com\"\n","\n","!git clone https://github.com/Deeppatel0510/Facial_Expression_Recognition.git\n","\n","!cp \"/content/drive/MyDrive/Master Degree Capstone/Facial_Expression_Recognition/Deep_Facial_Expression_Recognition.ipynb\" \"/content/Facial_Expression_Recognition/\"\n","\n","%cd /content/Facial_Expression_Recognition\n","\n","!git add Deep_Facial_Expression_Recognition.ipynb\n","!git commit -m \"Add notebook\"\n","\n","from google.colab import userdata\n","token = userdata.get('Git_Token')\n","\n","# Set the remote URL with the PAT for authentication\n","!git remote set-url origin https://Deeppatel0510:{token}@github.com/Deeppatel0510/Facial_Expression_Recognition.git\n","\n","# Push changes\n","!git push origin main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9lPHydhdagn"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNi9ZuJS6zfQUxMl+iCfnIu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}